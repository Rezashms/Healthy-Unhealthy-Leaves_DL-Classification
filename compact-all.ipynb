{"metadata":{"colab":{"provenance":[{"file_id":"1V0GYqG80q8PFRktNiSAXr_m0ZUqYK8vu","timestamp":1699815209869},{"file_id":"1V8LGND8fCIXqtZy6EFVJajsaKC5Ch16j","timestamp":1699791506685},{"file_id":"1emoiYjLDO5BiRFPebpt0eOu44oF5Vjc7","timestamp":1699739526776},{"file_id":"1z7PX0dHbpC3xLNzulA45YXBbOrg0yHmH","timestamp":1699731144386}],"gpuType":"T4","collapsed_sections":["WdIAHH3y1MGL","Px6y2YSkQb_C","P2ULXjC2r2G1","pua8lra2sVWf","Ub3zPKGT1fiy","KI7xuMEbZFPD","vYvf9mwNNnmf"]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":6946523,"sourceType":"datasetVersion","datasetId":3989497},{"sourceId":6997104,"sourceType":"datasetVersion","datasetId":4022200}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 1-Import Libraries","metadata":{"id":"WdIAHH3y1MGL"}},{"cell_type":"code","source":"import os, datetime, warnings, logging, random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport keras,cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom tensorflow import keras as tfk\nfrom tensorflow.keras import layers as tfkl\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers\nfrom tqdm.notebook import tqdm\nimport glob, random, time, shutil\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D,MaxPool2D, BatchNormalization, Activation, Input, Add, Dense, ZeroPadding2D,Flatten, AveragePooling2D, Rescaling, Dropout, MaxPooling2D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom keras import metrics\nfrom joblib import Parallel, delayed\nfrom sklearn.preprocessing import label_binarize, LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, Callback,ModelCheckpoint\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import regularizers","metadata":{"id":"qH4lmQYsrspb","execution":{"iopub.status.busy":"2023-11-18T16:25:22.856025Z","iopub.execute_input":"2023-11-18T16:25:22.856749Z","iopub.status.idle":"2023-11-18T16:25:22.865935Z","shell.execute_reply.started":"2023-11-18T16:25:22.856714Z","shell.execute_reply":"2023-11-18T16:25:22.864879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loading the model\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications import ResNet152\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.applications.resnet import preprocess_input as resnet_preprocess\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenetV2_preprocessing\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input as InceptionV3_preprocessing","metadata":{"execution":{"iopub.status.busy":"2023-11-18T16:25:22.868255Z","iopub.execute_input":"2023-11-18T16:25:22.868619Z","iopub.status.idle":"2023-11-18T16:25:22.883089Z","shell.execute_reply.started":"2023-11-18T16:25:22.868589Z","shell.execute_reply":"2023-11-18T16:25:22.882301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2-Fix randomness and hide warnings","metadata":{"id":"P2ULXjC2r2G1"}},{"cell_type":"code","source":"# Fix randomness and hide warnings\nSEED = 42\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['PYTHONHASHSEED'] = str(SEED)\nos.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=Warning)\n\nnp.random.seed(SEED)\n\nimport logging\n\nrandom.seed(SEED)","metadata":{"id":"oEYVRRdgMuWt","execution":{"iopub.status.busy":"2023-11-18T16:25:22.884159Z","iopub.execute_input":"2023-11-18T16:25:22.884430Z","iopub.status.idle":"2023-11-18T16:25:22.896274Z","shell.execute_reply.started":"2023-11-18T16:25:22.884399Z","shell.execute_reply":"2023-11-18T16:25:22.895466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3-Set first parameters of tensorflow","metadata":{"id":"pua8lra2sVWf"}},{"cell_type":"code","source":"# Import tensorflow\ntf.autograph.set_verbosity(0)\ntf.get_logger().setLevel(logging.ERROR)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\ntf.random.set_seed(SEED)\ntf.compat.v1.set_random_seed(SEED)\nprint(\"tensorflow_version\",tf.__version__)","metadata":{"id":"WC9nG5HlMvHd","outputId":"026ac360-b611-40ef-ae4c-0bba7b7e1679","executionInfo":{"status":"ok","timestamp":1699872511083,"user_tz":-60,"elapsed":574,"user":{"displayName":"romina saljooghian","userId":"01315879889988673809"}},"execution":{"iopub.status.busy":"2023-11-18T16:25:22.897275Z","iopub.execute_input":"2023-11-18T16:25:22.897521Z","iopub.status.idle":"2023-11-18T16:25:22.910900Z","shell.execute_reply.started":"2023-11-18T16:25:22.897500Z","shell.execute_reply":"2023-11-18T16:25:22.909919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4-Read DATA","metadata":{}},{"cell_type":"code","source":"dataset = np.load(\"/kaggle/input/a2ndl-farm/public_data.npz\", allow_pickle=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T16:25:26.123685Z","iopub.execute_input":"2023-11-18T16:25:26.124041Z","iopub.status.idle":"2023-11-18T16:25:26.179892Z","shell.execute_reply.started":"2023-11-18T16:25:26.124009Z","shell.execute_reply":"2023-11-18T16:25:26.178804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5-Data Inspection","metadata":{"id":"Ub3zPKGT1fiy"}},{"cell_type":"code","source":"keys = list(dataset.keys())\nprint('keys in our dataset are: ', keys)\n\n#data shape\nimages = dataset[\"data\"]\nno_images = images.shape[0]\nsize_images = images.shape[1:3]\nprint('Data shape: ',images.shape)\n\n#labels\nlabels = dataset[\"labels\"]\nno_labels = labels.shape[0]\n\n#check balanced or imbalanced of DATA\nunique_classes, counts = np.unique(labels,return_counts=True)\nfor cls, count in zip(unique_classes, counts):\n    print(f\"Class {cls}: {count} data points\")","metadata":{"id":"Ay-MIDxM1dvU","outputId":"b7d7527a-3eaf-4131-a12a-f34988e151a1","executionInfo":{"status":"ok","timestamp":1699872515623,"user_tz":-60,"elapsed":1690,"user":{"displayName":"romina saljooghian","userId":"01315879889988673809"}},"execution":{"iopub.status.busy":"2023-11-17T00:09:07.454844Z","iopub.execute_input":"2023-11-17T00:09:07.455173Z","iopub.status.idle":"2023-11-17T00:09:12.996408Z","shell.execute_reply.started":"2023-11-17T00:09:07.455150Z","shell.execute_reply":"2023-11-17T00:09:12.995314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6_Show images","metadata":{"id":"KI7xuMEbZFPD"}},{"cell_type":"code","source":"# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Fit and transform the string labels to integer labels\ninteger_labels = label_encoder.fit_transform(labels)\n\n# Convert integers to binary labels (0 or 1)\nbinary_labels = np.where(integer_labels == 1, 1, 0)","metadata":{"id":"eu4sX-LHAJji","execution":{"iopub.status.busy":"2023-11-17T00:09:12.997669Z","iopub.execute_input":"2023-11-17T00:09:12.997974Z","iopub.status.idle":"2023-11-17T00:09:13.005019Z","shell.execute_reply.started":"2023-11-17T00:09:12.997949Z","shell.execute_reply":"2023-11-17T00:09:13.003946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_images(dataset, labels, batch_no, no_images_per_batch, num_cols=10):\n\n    start_index = batch_no * no_images_per_batch\n    end_index = min((batch_no + 1) * no_images_per_batch, len(dataset))\n    num_rows = (end_index - start_index + num_cols - 1) // num_cols\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 10))\n\n    for i, ax in enumerate(axes.ravel()):\n        if start_index + i < end_index:\n            image = dataset[start_index + i]\n            image = image / 255.0  # Normalize pixel values to [0, 1]\n            ax.imshow(image)\n            ax.set_title(f\"{labels[start_index + i]}\",fontsize=10, y=-0.5)\n            ax.axis('off')\n\n    for i in range(end_index - start_index, num_rows * num_cols):\n        fig.delaxes(axes.flatten()[i])\n\n    plt.show()","metadata":{"id":"bf1SDKlIZbCg","execution":{"iopub.status.busy":"2023-11-17T00:09:13.006453Z","iopub.execute_input":"2023-11-17T00:09:13.006785Z","iopub.status.idle":"2023-11-17T00:09:13.017732Z","shell.execute_reply.started":"2023-11-17T00:09:13.006757Z","shell.execute_reply":"2023-11-17T00:09:13.017000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_images(dataset=images,labels =binary_labels, batch_no=0,no_images_per_batch=60)","metadata":{"id":"jWSVSRfKZdOl","outputId":"9ada903c-4153-4b11-8c95-85de73bd7bdb","executionInfo":{"status":"ok","timestamp":1699872526167,"user_tz":-60,"elapsed":4218,"user":{"displayName":"romina saljooghian","userId":"01315879889988673809"}},"execution":{"iopub.status.busy":"2023-11-17T00:09:13.018740Z","iopub.execute_input":"2023-11-17T00:09:13.019007Z","iopub.status.idle":"2023-11-17T00:09:15.172618Z","shell.execute_reply.started":"2023-11-17T00:09:13.018984Z","shell.execute_reply":"2023-11-17T00:09:15.170037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7-Remove Outliers","metadata":{"id":"vYvf9mwNNnmf"}},{"cell_type":"code","source":"# there are some uselees images in dataset which must be first recognized and then remoeved.\n\ndef histogram_similarity(image, reference_histogram, threshold=0.8):\n\n    # Calculate the color histogram of the image\n    image_histogram = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n    cv2.normalize(image_histogram, image_histogram)\n    intersection = cv2.compareHist(reference_histogram, image_histogram, cv2.HISTCMP_INTERSECT)\n\n    return intersection < threshold\n\n\n# Load a reference histogram from a non-Shrek-Singer image\nreference_image = images[0]  # Use the first image as a reference which it's a leaf image\nreference_histogram = cv2.calcHist([reference_image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\ncv2.normalize(reference_histogram, reference_histogram)\n\nsimilarity_threshold = 0.62\nodd_images = []\n\n# Iterate through the images in data_array\nfor image in images:\n    if histogram_similarity(image, reference_histogram, similarity_threshold):\n        odd_images.append(image)","metadata":{"id":"0EMa6m_1Nb9u","execution":{"iopub.status.busy":"2023-11-17T00:09:15.177729Z","iopub.execute_input":"2023-11-17T00:09:15.178411Z","iopub.status.idle":"2023-11-17T00:09:16.602739Z","shell.execute_reply.started":"2023-11-17T00:09:15.178376Z","shell.execute_reply":"2023-11-17T00:09:16.601858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating reference images\nreference_unwanted_images = []\n\n# Define the index of the image we want to plot and save as a reference\nimage_index_trololo = 10  # Replace with the index of the reference image\nimage_index_shrek = 33\n\nif 0 <= image_index_trololo  and image_index_shrek< len(odd_images):\n    # Get the image based on the index\n    trololo_reference_image = odd_images[image_index_trololo]\n    shrek_reference_image = odd_images[image_index_shrek]\n\n    reference_unwanted_images.append(trololo_reference_image)\n    reference_unwanted_images.append(shrek_reference_image)\n\nelse:\n    print(\"Invalid image index.\")","metadata":{"id":"flWTO2StPpOn","execution":{"iopub.status.busy":"2023-11-17T00:09:16.603894Z","iopub.execute_input":"2023-11-17T00:09:16.604220Z","iopub.status.idle":"2023-11-17T00:09:16.610699Z","shell.execute_reply.started":"2023-11-17T00:09:16.604194Z","shell.execute_reply":"2023-11-17T00:09:16.609718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skimage.color import rgb2lab, deltaE_cie76\n\n#Calculate the color distance between two images using the CIE76 metric.\ndef calculate_color_distance(image1, image2):\n\n    lab_image1 = rgb2lab(image1)\n    lab_image2 = rgb2lab(image2)\n\n    return deltaE_cie76(lab_image1, lab_image2)\n\ndef find_unwanted_images_by_color_distance(data_array, reference_unwanted_images, color_distance_threshold):\n\n    cleaned_data = []\n    unwanted_images = []\n    labels_index =[]\n    outlier_labels_index=[]\n    index =0\n    for image in data_array:\n        similar = False\n        for reference_image in reference_unwanted_images:\n            color_distance = calculate_color_distance(image, reference_image)\n            if (color_distance < color_distance_threshold).all():\n                similar = True\n                break  # No need to check further if a match is found\n        if (similar == True):\n            unwanted_images.append(image)\n            labels_index.append(index)\n        else:\n            cleaned_data.append(image)\n            outlier_labels_index.append(index)\n\n        index+=1\n\n    return cleaned_data,unwanted_images, labels_index\n\ncolor_distance_threshold = 1","metadata":{"id":"_phUYj8UP6tP","execution":{"iopub.status.busy":"2023-11-17T00:09:16.611998Z","iopub.execute_input":"2023-11-17T00:09:16.612330Z","iopub.status.idle":"2023-11-17T00:09:16.680128Z","shell.execute_reply.started":"2023-11-17T00:09:16.612299Z","shell.execute_reply":"2023-11-17T00:09:16.679219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_images,unwanted_images, labels_index = find_unwanted_images_by_color_distance(images, reference_unwanted_images, color_distance_threshold)","metadata":{"id":"YMTgF4kZQEAG","execution":{"iopub.status.busy":"2023-11-17T00:09:16.681212Z","iopub.execute_input":"2023-11-17T00:09:16.681529Z","iopub.status.idle":"2023-11-17T00:09:33.761014Z","shell.execute_reply.started":"2023-11-17T00:09:16.681502Z","shell.execute_reply":"2023-11-17T00:09:33.759927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unwanted_images = np.stack(unwanted_images, axis=0) # Define new shape for unwanted_images\nunwanted_images.shape","metadata":{"id":"GbJgZNUuQQfW","outputId":"a6ac4564-b28f-4521-bacd-b2b132c38a7f","executionInfo":{"status":"ok","timestamp":1699872581325,"user_tz":-60,"elapsed":8,"user":{"displayName":"romina saljooghian","userId":"01315879889988673809"}},"execution":{"iopub.status.busy":"2023-11-17T00:09:33.762449Z","iopub.execute_input":"2023-11-17T00:09:33.762776Z","iopub.status.idle":"2023-11-17T00:09:33.778313Z","shell.execute_reply.started":"2023-11-17T00:09:33.762748Z","shell.execute_reply":"2023-11-17T00:09:33.777290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_images = np.stack(cleaned_images, axis=0)\ncleaned_images.shape","metadata":{"id":"DT_A9Ie3RbjP","outputId":"3ea8a540-2aee-4d26-a68d-ad69be76445e","executionInfo":{"status":"ok","timestamp":1699872581975,"user_tz":-60,"elapsed":656,"user":{"displayName":"romina saljooghian","userId":"01315879889988673809"}},"execution":{"iopub.status.busy":"2023-11-17T00:09:33.779677Z","iopub.execute_input":"2023-11-17T00:09:33.780118Z","iopub.status.idle":"2023-11-17T00:09:33.966180Z","shell.execute_reply.started":"2023-11-17T00:09:33.780069Z","shell.execute_reply":"2023-11-17T00:09:33.965195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a boolean mask for the values to keep\nmask = np.ones(binary_labels.shape[0], dtype=bool)\nmask[labels_index] = False\n\ncleaned_labels = binary_labels[mask]\noutlier_labels=binary_labels[labels_index]","metadata":{"id":"C1gJ81UdRjc4","execution":{"iopub.status.busy":"2023-11-17T00:09:33.967533Z","iopub.execute_input":"2023-11-17T00:09:33.967952Z","iopub.status.idle":"2023-11-17T00:09:33.973110Z","shell.execute_reply.started":"2023-11-17T00:09:33.967922Z","shell.execute_reply":"2023-11-17T00:09:33.972185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\" \"*25,end=\" \")\nprint(\"Unwanted Images \",end=' '*25)\nshow_images(dataset=unwanted_images,labels=outlier_labels, batch_no=0,no_images_per_batch=10)","metadata":{"id":"fLjmSabQ3-79","executionInfo":{"status":"ok","timestamp":1699872582468,"user_tz":-60,"elapsed":504,"user":{"displayName":"romina saljooghian","userId":"01315879889988673809"}},"outputId":"7b6a3b59-3a20-4e8e-af7f-aa7c242457ce","execution":{"iopub.status.busy":"2023-11-17T00:09:33.974198Z","iopub.execute_input":"2023-11-17T00:09:33.974603Z","iopub.status.idle":"2023-11-17T00:09:34.341566Z","shell.execute_reply.started":"2023-11-17T00:09:33.974570Z","shell.execute_reply":"2023-11-17T00:09:34.340026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\" \"*25,end=\" \")\nprint(\"Cleaned Images \",end=' '*25)\nshow_images(dataset=cleaned_images,labels =cleaned_labels, batch_no=0,no_images_per_batch=10)","metadata":{"id":"qWPSJzZV4vxw","executionInfo":{"status":"ok","timestamp":1699872583223,"user_tz":-60,"elapsed":790,"user":{"displayName":"romina saljooghian","userId":"01315879889988673809"}},"outputId":"3abd0d63-ea0f-4f85-8f3c-bd894feb3cae","execution":{"iopub.status.busy":"2023-11-17T00:09:34.344125Z","iopub.execute_input":"2023-11-17T00:09:34.345099Z","iopub.status.idle":"2023-11-17T00:09:34.751404Z","shell.execute_reply.started":"2023-11-17T00:09:34.345050Z","shell.execute_reply":"2023-11-17T00:09:34.749778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_images = cleaned_images.shape[0]\nno_labels = cleaned_labels.shape[0]\n_, counts = np.unique(cleaned_labels,return_counts=True) # count occurrence of each item\nno_healthy_images = counts[0]\nno_unhealthy_images = counts[1]\n\n# pass variables to a dictionary to be used as dataframe for a better show\ninfo_table_dict = {\"no. images\":no_images, \"image width\": size_images[0],\"image length\": size_images[1], \"no. labels\":no_labels,\n                   \"no. healthy_images\":no_healthy_images,\"percentage%\":no_healthy_images*100/no_labels,\n                   \"no. unhealthy_images\":no_unhealthy_images,\"percentage %\":no_unhealthy_images*100/no_labels }\nprint(\" \"*42,end=\" \")\nprint(\"Table after Removing Outliers \",end=' '*42)\ninfo_table = pd.DataFrame(info_table_dict, index =['value'])\ninfo_table","metadata":{"id":"OJM58pT52Fd1","outputId":"139195b8-e727-4d77-b51d-c2f50d67898e","executionInfo":{"status":"ok","timestamp":1699872583225,"user_tz":-60,"elapsed":41,"user":{"displayName":"romina saljooghian","userId":"01315879889988673809"}},"execution":{"iopub.status.busy":"2023-11-17T00:09:34.753844Z","iopub.execute_input":"2023-11-17T00:09:34.754353Z","iopub.status.idle":"2023-11-17T00:09:34.794130Z","shell.execute_reply.started":"2023-11-17T00:09:34.754307Z","shell.execute_reply":"2023-11-17T00:09:34.793129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8-Class Weight","metadata":{}},{"cell_type":"code","source":"# Calculate class frequencies\nclass_frequencies = np.sum(np.array(category_labels), axis=0)\n\n# Calculate class weights inversely proportional to class frequencies\ntotal_samples = np.sum(class_frequencies)\nclass_weights = {i: total_samples / (len(class_frequencies) * freq) for i, freq in enumerate(class_frequencies)}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9-Splitting data And Augmentation","metadata":{"id":"PrB7sWt7urCD"}},{"cell_type":"code","source":"BATCH_SIZE = 64\ncategory_labels = to_categorical(cleaned_labels, num_classes=2)","metadata":{"id":"aHR9CiUCy5qz","execution":{"iopub.status.busy":"2023-11-17T00:09:34.795333Z","iopub.execute_input":"2023-11-17T00:09:34.795592Z","iopub.status.idle":"2023-11-17T00:09:34.800619Z","shell.execute_reply.started":"2023-11-17T00:09:34.795569Z","shell.execute_reply":"2023-11-17T00:09:34.799657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constructor for training data generator with augmentation\ndata_datagen = ImageDataGenerator(\n    rotation_range=30,\n#     height_shift_range=0.2,\n#     width_shift_range=0.2,\n    zoom_range=[0.7 , 1.3],\n    horizontal_flip=True,\n    vertical_flip=True,\n    brightness_range=[0.8, 1.2],\n    fill_mode='reflect',\n    rescale=1./255,\n    # change this based on the network you want use!!!\n    # default : CNN e.g. 'resnet_preprocess'\n    validation_split=0.1,\n    preprocessing_function=None\n)\n\n# # Constructor for validation data generator without augmentation comment for TTA\n# data_datagen_val = ImageDataGenerator(\n#     rescale=1./255,\n#     validation_split=0.1,  # Use the same validation split\n#     preprocessing_function=None\n# )\n\n# Generator for training set\naug_train_set = data_datagen.flow(\n    cleaned_images,\n    category_labels,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=SEED,\n    subset=\"training\"\n)\n\n# Generator for validation set (with augmentation)\nvalidation_set = data_datagen.flow(\n    cleaned_images,\n    category_labels,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=SEED,\n    subset=\"validation\"\n)","metadata":{"id":"RXEsdB1H7brg","execution":{"iopub.status.busy":"2023-11-17T00:09:34.802369Z","iopub.execute_input":"2023-11-17T00:09:34.803329Z","iopub.status.idle":"2023-11-17T00:09:34.815786Z","shell.execute_reply.started":"2023-11-17T00:09:34.803301Z","shell.execute_reply":"2023-11-17T00:09:34.815031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10-Define Models","metadata":{"id":"YGXwMsMqV5ha"}},{"cell_type":"markdown","source":"**CNN**","metadata":{}},{"cell_type":"code","source":"def CNN_model(hp):\n    model = tf.keras.models.Sequential([\n          tf.keras.layers.Conv2D(filters=hp.Int('conv1_filters', min_value=32, max_value=128, step=16),\n                                 kernel_size=hp.Choice('conv1_kernel', values=[3, 5]), activation='elu', input_shape=(96, 96, 3)),\n          tf.keras.layers.BatchNormalization(renorm=True),\n          tf.keras.layers.MaxPooling2D(2,2),\n          tf.keras.layers.Conv2D(filters=hp.Int('conv2_filters', min_value=64, max_value=128, step=32),\n                                 kernel_size=hp.Choice('conv2_kernel', values=[3, 5]), activation='elu'),\n          tf.keras.layers.Conv2D(filters=hp.Int('conv3_filters', min_value=64, max_value=256, step=32),\n                                 kernel_size=hp.Choice('conv3_kernel', values=[3, 5]), activation='elu'),\n          tf.keras.layers.BatchNormalization(renorm=True),\n          tf.keras.layers.MaxPooling2D(2,2),\n          tf.keras.layers.Conv2D(filters=hp.Int('conv4_filters', min_value=64, max_value=512, step=32),\n                                 kernel_size=hp.Choice('conv4_kernel', values=[3, 5]), activation='elu'),\n          tf.keras.layers.Conv2D(filters=hp.Int('conv5_filters', min_value=128, max_value=1024, step=64),\n                                 kernel_size=hp.Choice('conv5_kernel', values=[3, 5]), activation='elu'),\n          tf.keras.layers.BatchNormalization(renorm=True),\n          tf.keras.layers.Conv2D(filters=hp.Int('conv6_filters', min_value=64, max_value=512, step=64),\n                                 kernel_size=hp.Choice('conv6_kernel', values=[3, 5]), activation='elu'),\n          tf.keras.layers.Conv2D(filters=hp.Int('conv7_filters', min_value=32, max_value=256, step=16),\n                                 kernel_size=hp.Choice('conv7_kernel', values=[3, 5]), activation='elu'),\n          tf.keras.layers.GlobalMaxPooling2D(),\n          tf.keras.layers.Flatten(),\n          tf.keras.layers.Dropout(rate=hp.Float('dropout1_rate', min_value=0.05, max_value=0.2, step=0.05),seed=SEED),\n          tf.keras.layers.Dense(units=hp.Int('dense1_units', min_value=64, max_value=512, step=32),activation= None),\n          tf.keras.layers.Dropout(rate=hp.Float('dropout2_rate', min_value=0.4, max_value=0.6, step=0.1),seed=SEED),\n          tf.keras.layers.Dense(units=hp.Int('dense2_units', min_value=32, max_value=256, step=16),activation= None),\n          tf.keras.layers.Dense(units=hp.Int('dense3_units', min_value=32, max_value=128, step=8),\n                                activation=keras.layers.LeakyReLU(alpha=hp.Float('leaky_relu_alpha', min_value=0.01, max_value=0.3, step=0.01))),\n          tf.keras.layers.Dense(2, activation='softmax')\n          ])\n\n    opt = tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-3, 5e-4, 1e-4]))\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model","metadata":{"id":"R9osAfoqiPHr","execution":{"iopub.status.busy":"2023-11-17T00:09:34.817112Z","iopub.execute_input":"2023-11-17T00:09:34.817732Z","iopub.status.idle":"2023-11-17T00:09:34.836796Z","shell.execute_reply.started":"2023-11-17T00:09:34.817695Z","shell.execute_reply":"2023-11-17T00:09:34.836057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility function for callbacks for training\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 5 , verbose=1,factor=0.3, min_lr=0.000001)\n# Model checkpoint\nmcp_save_CNN = ModelCheckpoint('model_CNN.hdf5', save_best_only=True, monitor='val_loss', mode='min')","metadata":{"id":"d3cfbcef","execution":{"iopub.status.busy":"2023-11-17T00:09:34.857258Z","iopub.execute_input":"2023-11-17T00:09:34.857611Z","iopub.status.idle":"2023-11-17T00:09:34.867521Z","shell.execute_reply.started":"2023-11-17T00:09:34.857569Z","shell.execute_reply":"2023-11-17T00:09:34.866753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history_CNN_simple = model_CNN.fit(aug_train_set,\n#     epochs=50,\n#     verbose=1,\n#     callbacks=[learning_rate_reduction, mcp_save_CNN],\n#     validation_data= validation_set\n# )","metadata":{"id":"cxswhQJAiq7U","execution":{"iopub.status.busy":"2023-11-17T00:09:34.868668Z","iopub.execute_input":"2023-11-17T00:09:34.868981Z","iopub.status.idle":"2023-11-17T00:09:34.879008Z","shell.execute_reply.started":"2023-11-17T00:09:34.868957Z","shell.execute_reply":"2023-11-17T00:09:34.878190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history_CNN_deleteoutliers = model_CNN.fit(aug_train_set,\n#     epochs=50,\n#     verbose=1,\n#     callbacks=[learning_rate_reduction, mcp_save_CNN],\n#     validation_data= validation_set\n# )","metadata":{"id":"-QyZtfSQzR6e","execution":{"iopub.status.busy":"2023-11-17T00:09:34.879896Z","iopub.execute_input":"2023-11-17T00:09:34.880155Z","iopub.status.idle":"2023-11-17T00:09:34.890702Z","shell.execute_reply.started":"2023-11-17T00:09:34.880124Z","shell.execute_reply":"2023-11-17T00:09:34.889766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history_CNN_Aug = loaded_model.fit(aug_train_set,\n#     epochs=50,\n#     verbose=1,\n#     callbacks=[learning_rate_reduction, mcp_save_CNN],\n#     validation_data= validation_set\n# )","metadata":{"id":"aQgwOfRw3DPt","execution":{"iopub.status.busy":"2023-11-17T00:09:34.891969Z","iopub.execute_input":"2023-11-17T00:09:34.892846Z","iopub.status.idle":"2023-11-17T00:09:34.904452Z","shell.execute_reply.started":"2023-11-17T00:09:34.892816Z","shell.execute_reply":"2023-11-17T00:09:34.903605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner = RandomSearch(\n    CNN_model,\n    objective='val_accuracy',\n    max_trials=10,  # تعداد تلاش‌ها برای جستجو\n    directory='/kaggle/working',  # مسیر ذخیره سازی نتایج\n    project_name='cnn_tuning'  # نام پروژه\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T00:09:34.909732Z","iopub.execute_input":"2023-11-17T00:09:34.910051Z","iopub.status.idle":"2023-11-17T00:09:40.704245Z","shell.execute_reply.started":"2023-11-17T00:09:34.910020Z","shell.execute_reply":"2023-11-17T00:09:40.703195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner.search(aug_train_set,\n    epochs=50,\n    verbose=1,\n    callbacks=[learning_rate_reduction, mcp_save_CNN],\n    validation_data= validation_set,\n    class_weight=class_weights\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T00:09:40.714510Z","iopub.execute_input":"2023-11-17T00:09:40.714879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = tuner.get_best_models(num_models=1)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model.save('/kaggle/working/SubmissionModel')\n\nfrom shutil import make_archive\nmake_archive('/kaggle/working/SubmissionModel_CNN', 'zip', '/kaggle/working/SubmissionModel')","metadata":{"id":"rH-MD9FQIDE8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with open(BASE_DIR+'history_CNN_Aug.pkl', 'wb') as file:\n#     pickle.dump(history_CNN_Aug.history, file)\n# with open(BASE_DIR+'history_CNN_simple.pkl', 'rb') as file:\n#     history_CNN_simple = pickle.load(file)\n# with open(BASE_DIR+'history_CNN_deleteoutliers.pkl', 'rb') as file:\n#     history_CNN_deleteoutliers = pickle.load(file)\n# with open(BASE_DIR+'history_CNN_Aug.pkl', 'rb') as file:\n#     history_CNN_Aug = pickle.load(file)","metadata":{"id":"966KY-A-6U3h","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #plotting the validation and train loss\n# plt.plot(history_CNN_simple['loss'], alpha=.3, color='#ff7f0e', linestyle='--')\n# plt.plot(history_CNN_simple['val_loss'], label='CNN', alpha=.8, color='#ff7f0e')\n# plt.plot(history_CNN_deleteoutliers['loss'], alpha=.3, color='#4D61E2', linestyle='--')\n# plt.plot(history_CNN_deleteoutliers['val_loss'], label='CNN_Clean', alpha=.8, color='#4D61E2')\n# plt.plot(history_CNN_Aug['loss'], alpha=.3, color='#4de2a1', linestyle='--')\n# plt.plot(history_CNN_Aug['val_loss'], label='CNN_Aug', alpha=.8, color='#4de2a1')\n# plt.title('Categorical Crossentropy')\n# plt.grid(alpha=.3)\n# plt.ylabel('loss')\n# plt.xlabel('epoch')\n# plt.legend(loc='upper left')\n# plt.savefig('Categorical Crossentropy.jpg')\n# plt.show()","metadata":{"id":"suOuIw_7S0wM","executionInfo":{"status":"ok","timestamp":1699872344882,"user_tz":-60,"elapsed":1793,"user":{"displayName":"romina saljooghian","userId":"01315879889988673809"}},"outputId":"824db93c-6a3d-48f8-8ea9-4ef3857254ad","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #plotting the validation and train ACC\n# plt.plot(history_CNN_simple['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n# plt.plot(history_CNN_simple['val_accuracy'], label='simple CNN', alpha=.8, color='#ff7f0e')\n# plt.plot(history_CNN_deleteoutliers['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')\n# plt.plot(history_CNN_deleteoutliers['val_accuracy'], label='CNN_Clean', alpha=.8, color='#4D61E2')\n# plt.plot(history_CNN_Aug['accuracy'], alpha=.3, color='#4de2a1', linestyle='--')\n# plt.plot(history_CNN_Aug['val_accuracy'], label='CNN_Aug', alpha=.8, color='#4de2a1')\n# plt.title('model accuracy')\n# plt.grid(alpha=.3)\n# plt.legend(loc='upper left')\n# plt.savefig('Model Accuracy.jpg')\n# plt.show()","metadata":{"id":"9UD8PMZQS9-J","executionInfo":{"status":"ok","timestamp":1699872351375,"user_tz":-60,"elapsed":924,"user":{"displayName":"romina saljooghian","userId":"01315879889988673809"}},"outputId":"318b17a2-6358-41ad-8ac3-abdb70ee998b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MobileNET**","metadata":{}},{"cell_type":"code","source":"# Create MobileNetV2 model with specified settings\ninput_shape = (96, 96, 3)\nmobile = tfk.applications.MobileNetV2(\n    input_shape=input_shape,\n    include_top=False,\n    weights=\"imagenet\",\n    pooling='avg',\n)\n# tfk.utils.plot_model(mobile, show_shapes=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an input layer based on your input shape\ninput_layer = tfkl.Input(shape=input_shape)\n\n# Connect MobileNetV2 to the input layer\nx = mobile(input_layer)\n\nx = tfkl.Dropout(0.5)(x)\nx = tfkl.Dense(512, activation=None, kernel_initializer = tf.keras.initializers.HeUniform(SEED))(x)\nx = tfkl.Dropout(0.5)(x)\nx = tfkl.Dense(128, activation='leaky_relu', kernel_initializer = tf.keras.initializers.HeUniform(SEED))(x)\n\n# Add a Dense layer with 2 units and softmax activation as the classifier\noutputs = tfkl.Dense(2, activation='softmax' , kernel_initializer=keras.initializers.HeUniform(SEED) )(x)\n\n# Create a Model connecting input and output\nMobileNetV2_model = tf.keras.Model(inputs=input_layer, outputs=outputs, name='model')\n\n# make it true for FT-\nMobileNetV2_model.get_layer('mobilenetv2_1.00_96').trainable = False\n\n# for i, layer in enumerate(MobileNetV2_model.get_layer('mobilenetv2_1.00_96').layers[:51]):\n#   layer.trainable=False\n\n# Compile the model with Categorical Cross-Entropy loss and Adam optimizer\nMobileNetV2_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n\n# Display model summary\nMobileNetV2_model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility function for callbacks for training\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 10, verbose=1,factor=0.2, min_lr=0.000001)\n# Model checkpoint\nmcp_save_model_mobile = ModelCheckpoint('MobileNetV2_model.hdf5', save_best_only=True, monitor='accuracy', mode='max')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory_MobileNetV2_model = MobileNetV2_model.fit(\n    aug_train_set,\n    steps_per_epoch=len(aug_train_set),\n    epochs=50,\n    verbose=1,\n    callbacks=[learning_rate_reduction, mcp_save_model_mobile],\n    validation_data= validation_set,\n#     class_weight=class_weights\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting the validation and train loss\nplt.plot(history_MobileNetV2_model.history['loss'])\nplt.plot(history_MobileNetV2_model.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting the validation and train ACC\nplt.plot(history_MobileNetV2_model.history['accuracy'])\nplt.plot(history_MobileNetV2_model.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.savefig('Model Accuracy_MobileNet.jpg')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n\ntrue_labels = category_labels[validation_set.index_array]\ntrue_labels = true_labels.astype(int)\n\nprediction =MobileNetV2_model.predict(validation_set)\nbinary_predictions = (prediction > 0.5).astype(int)\n\ncm = classification_report(true_labels, binary_predictions)\nprint(cm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MobileNetV2_model.save('/kaggle/working/SubmissionModel')\n\nfrom shutil import make_archive\nmake_archive('/kaggle/working/SubmissionModel_MobileNet', 'zip', '/kaggle/working/SubmissionModel')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ResNet50**","metadata":{}},{"cell_type":"code","source":"def resnet50_model(Hyparam):\n    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n    GAP = layers.GlobalAveragePooling2D()\n    flatten_layer = tf.keras.layers.Flatten()\n    dropout = tf.keras.layers.Dropout(0.2)\n    dense_layer = layers.Dense(Hyparam, activation='relu', kernel_initializer = tf.keras.initializers.HeUniform(SEED))\n    dropout_1 = tf.keras.layers.Dropout(0.5)\n    prediction_layer = layers.Dense(2, activation='softmax')\n    batch_norm = layers.BatchNormalization()\n    Relu = layers.ReLU()\n    for layer in base_model.layers[:40]:\n        layer.trainable = False\n    model = Sequential([\n    base_model,\n    GAP,\n    dropout,\n    dense_layer,\n    batch_norm,\n    dropout_1,\n    prediction_layer\n    ])\n    opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_resnet50=resnet50_model(128)\nmodel_resnet50.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility function for callbacks for training\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 3, verbose=1,factor=0.3, min_lr=0.000001)\n# Model checkpoint\nmcp_save_ResNet50 = ModelCheckpoint('model_ResNet50.hdf5', save_best_only=True, monitor='val_loss', mode='min')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_ResNet50 = model_resnet50.fit(\n    aug_train_set,\n    steps_per_epoch=len(aug_train_set),\n    epochs=50,\n    verbose=1,\n    callbacks=[learning_rate_reduction, mcp_save_ResNet50],\n    validation_data= validation_set\n#     class_weight=class_weights\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting the validation and train loss\nplt.plot(history_ResNet50.history['loss'])\nplt.plot(history_ResNet50.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting the validation and train ACC\nplt.plot(history_ResNet50.history['accuracy'])\nplt.plot(history_ResNet50.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.savefig('Model Accuracy_ResNet.jpg')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n\ntrue_labels = category_labels[validation_set.index_array]\ntrue_labels = true_labels.astype(int)\n\nprediction =model_resnet50.predict(validation_set)\nbinary_predictions = (prediction > 0.5).astype(int)\n\ncm = classification_report(true_labels, binary_predictions)\nprint(cm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_resnet50.save('/kaggle/working/SubmissionModel')\n\nfrom shutil import make_archive\nmake_archive('/kaggle/working/SubmissionModel_ResNet50', 'zip', '/kaggle/working/SubmissionModel')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**EfficientnetB0**","metadata":{}},{"cell_type":"code","source":"def efficientnetB0_model(Hyparam,Hyparam_1):\n    # Load the pre-trained EfficientNetB0 model with weights from ImageNet\n    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n    GAP = layers.GlobalAveragePooling2D()\n    flatten_layer = tf.keras.layers.Flatten()\n    dropout = tf.keras.layers.Dropout(0.2,seed=SEED)\n    dense_layer = layers.Dense(Hyparam, activation=None, kernel_initializer = tf.keras.initializers.HeUniform(SEED))\n    dropout_1 = tf.keras.layers.Dropout(0.5,seed=SEED)\n    dense_layer_1 = layers.Dense(Hyparam_1, activation=None, kernel_initializer = tf.keras.initializers.HeUniform(SEED))\n    dropout_2 = tf.keras.layers.Dropout(0.1,seed=SEED)\n    prediction_layer = layers.Dense(2, activation='softmax')\n    batch_norm = layers.BatchNormalization()\n    batch_norm_1 = layers.BatchNormalization()\n    Relu = layers.ReLU()\n    for layer in base_model.layers[:90]:\n        layer.trainable = False\n    model = Sequential([\n    base_model,\n    GAP,\n    dropout,\n    dense_layer,\n    batch_norm,\n    dropout_1,\n    dense_layer_1,\n    batch_norm_1,\n    dropout_2,\n    prediction_layer\n    ])\n    opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_efficientnetB0 = efficientnetB0_model(512,256)\nmodel_efficientnetB0.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility function for callbacks for training\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 5, verbose=1,factor=0.3, min_lr=0.000001)\n# Model checkpoint\nmcp_save_EfficientnetB0 = ModelCheckpoint('model_efficientnetB0.hdf5', save_best_only=True, monitor='val_loss', mode='min')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_efficientnetB0 = model_efficientnetB0.fit(\n    aug_train_set,\n    steps_per_epoch=len(aug_train_set),\n    epochs=60,\n    verbose=1,\n    callbacks=[mcp_save_EfficientnetB0, mcp_save_ResNet50],\n    validation_data= validation_set,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting the validation and train loss\nplt.plot(history_efficientnetB0.history['loss'])\nplt.plot(history_efficientnetB0.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting the validation and train ACC\nplt.plot(history_efficientnetB0.history['accuracy'])\nplt.plot(history_efficientnetB0.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.savefig('Model Accuracy_Efficientnet.jpg')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n\ntrue_labels = category_labels[validation_set.index_array]\ntrue_labels = true_labels.astype(int)\n\nprediction =model_efficientnetB0.predict(validation_set)\nbinary_predictions = (prediction > 0.5).astype(int)\n\ncm = classification_report(true_labels, binary_predictions)\nprint(cm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_efficientnetB0.save('/kaggle/working/SubmissionModel')\n\nfrom shutil import make_archive\nmake_archive('/kaggle/working/SubmissionModel_Efficien', 'zip', '/kaggle/working/SubmissionModel')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**InceptionV3**","metadata":{}},{"cell_type":"code","source":"# Create InceptionV3 model with specified settings\ninput_shape = (96, 96, 3)\nmobile = tfk.applications.InceptionV3(\n    input_shape=input_shape,\n    include_top=False,\n    weights=\"imagenet\",\n    pooling='avg',\n)\n# tfk.utils.plot_model(mobile, show_shapes=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an input layer based on your input shape\ninput_layer = tfkl.Input(shape=input_shape)\n\n# Connect MobileNetV2 to the input layer\nx = mobile(input_layer)\nx = tfkl.Dropout(0.2)(x)\nx = tfkl.Dense(512, activation=None, kernel_initializer = tf.keras.initializers.HeUniform(SEED))(x)\nx = tfkl.BatchNormalization()(x)\nx = tfkl.Dropout(0.5)(x)\nx = tfkl.Dense(128, activation=None, kernel_initializer = tf.keras.initializers.HeUniform(SEED))(x)\nx = tfkl.Dropout(0.1)(x)\nx = tfkl.Dense(64, activation=None, kernel_initializer = tf.keras.initializers.HeUniform(SEED))(x)\n\n\n# Add a Dense layer with 2 units and softmax activation as the classifier\noutputs = tfkl.Dense(2, activation='softmax')(x)\n\n# Create a Model connecting input and output\nInceptionV3_model = tf.keras.Model(inputs=input_layer, outputs=outputs, name='model')\n\nInceptionV3_model.get_layer('inception_v3').trainable = True\nfor i, layer in enumerate(InceptionV3_model.get_layer('inception_v3').layers[:38]):\n  layer.trainable=False\n\n# Compile the model with Categorical Cross-Entropy loss and Adam optimizer\nInceptionV3_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n\n# Display model summary\nInceptionV3_model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility function for callbacks for training\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 5, verbose=1,factor=0.3, min_lr=0.000001)\n# Model checkpoint\nmcp_save_InceptionV3 = ModelCheckpoint('InceptionV3_model.hdf5', save_best_only=True, monitor='val_loss', mode='min')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory_InceptionV3_model = InceptionV3_model.fit(\n    aug_train_set,\n    epochs=60,\n    verbose=1,\n    callbacks=[learning_rate_reduction, mcp_save_InceptionV3],\n    validation_data= validation_set,\n    class_weight=class_weights\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting the validation and train loss\nplt.plot(history_InceptionV3_model.history['loss'])\nplt.plot(history_InceptionV3_model.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting the validation and train ACC\nplt.plot(history_InceptionV3_model.history['accuracy'])\nplt.plot(history_InceptionV3_model.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.savefig('Model Accuracy_InceptionV3.jpg')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n\ntrue_labels = category_labels[validation_set.index_array]\ntrue_labels = true_labels.astype(int)\n\nprediction =InceptionV3_model.predict(validation_set)\nbinary_predictions = (prediction > 0.5).astype(int)\n\ncm = classification_report(true_labels, binary_predictions)\nprint(cm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InceptionV3_model.save('/kaggle/working/SubmissionModel')\n\nfrom shutil import make_archive\nmake_archive('/kaggle/working/SubmissionModel_InceptionV3', 'zip', '/kaggle/working/SubmissionModel')","metadata":{},"execution_count":null,"outputs":[]}]}